options:
  defaultLogsBucketBehavior: REGIONAL_USER_OWNED_BUCKET

steps:
  # Step 1: Run Scrapy spider and save data
  - name: 'python:3.10'
    id: 'Run Scrapy Spider'
    entrypoint: bash
    args:
      - -c
      - |
        pip install --upgrade pip
        pip install -r requirements.txt
        mkdir -p /workspace/data
        cd lulu_pipeline     # <-- move into folder with scrapy.cfg
        scrapy crawl lulu -o /workspace/data/products.json

  # Step 2: Upload scraped data to GCS (raw folder)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Upload Data to GCS'
    entrypoint: bash
    args:
      - -c
      - |
        gsutil cp -r /workspace/data/* gs://scraped-data-bucket-test/raw/
