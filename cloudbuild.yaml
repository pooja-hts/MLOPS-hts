options:
  defaultLogsBucketBehavior: REGIONAL_USER_OWNED_BUCKET

steps:
  # Step 1: Run Scrapy spider and save data
  - name: 'python:3.10'
    id: 'Run Scrapy Spider'
    entrypoint: bash
    args:
      - -c
      - |
        set -e
        pip install --upgrade pip
        pip install -r requirements.txt
        mkdir -p /workspace/data
        # Run scrapy from repo root (scrapy.cfg is here)
        scrapy crawl lulu -o /workspace/data/lulu_output.json

  # Step 2: Upload scraped data to GCS (raw folder)
  - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
    id: 'Upload Data to GCS'
    entrypoint: bash
    args:
      - -c
      - |
        gsutil cp -r /workspace/data/* gs://scraped-data-bucket-test/raw/
